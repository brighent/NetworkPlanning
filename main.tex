\documentclass[conference]{IEEEtran}


\usepackage[utf8]{inputenc}
\usepackage[linesnumbered,lined, algoruled]{algorithm2e}
\usepackage{algorithmic}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{balance}
\usepackage{bm}
\usepackage{color,soul}
%\usepackage{epstopdf}
\usepackage[acronym,shortcuts]{glossaries}
\usepackage{graphicx}
\usepackage{graphics}
%\usepackage[inline]{enumitem}
\makeglossaries
%%% Glossaries/Acronyms


\newacronym{auc}{AUC}{area under the curve}
\newacronym{bs}{AP}{access point}
\newacronym{ce}{CE}{cross entropy}
\newacronym{cdf}{CDF}{cumulative distribution function}
\newacronym{fa}{FA}{false alarm}
\newacronym{kl}{K-L}{Kullback-Leibler}
\newacronym{ls}{LS}{least-squares}
\newacronym{llr}{LLR}{log likelihood-ratio}
\newacronym{los}{LOS}{line of sight}
\newacronym{lssvm}{LS-SVM}{least squares SVM}
\newacronym{md}{MD}{mis-detection}
\newacronym{ml}{ML}{machine learning}
\newacronym{mlp}{MLP}{multy-layer perceptron}
\newacronym{mse}{MSE}{mean squared error}
\newacronym[\glslongpluralkey={neural networks}]{nn}{NN}{neural network}
\newacronym{np}{N-P}{Neyman-Pearson}
\newacronym{oclssvm}{OCLSSVM}{one-class least-square \ac{svm}}
\newacronym{pdf}{PDF}{probability density function}
\newacronym{pmd}{PMD}{probability mass distribution}
\newacronym{pso}{PSO}{particle swarm optimization}
\newacronym{rnn}{RNN}{replicator neural network}
\newacronym{roc}{ROC}{receiver operating characteristic}
\newacronym{rss}{RSS}{received signal strength}
\newacronym[\glslongpluralkey={support vector machines}]{svm}{SVM}{support vector machine}
\newacronym{ue}{UE}{user equipment}
\newacronym{wsn}{WSN}{wireless sensor network}
\newacronym{irlv}{IRLV}{in-region location verification}

\newcommand{\cross}{H_{p_t}(p_{\mathcal{H}_0|\bf y})}
\newcommand{\hatcross}{\hat{H}(p_t,p_{\mathcal{H}_0|\bf y})}

%\usepackage[autostyle]{csquotes}
\usepackage[backend=biber,style=ieee]{biblatex}
\bibliography{bibliography2.bib}


\title{Location-Verification and Network Planning \\ via Machine Learning Approaches}
\author{Alessandro Brighente, Francesco Formaggio, Marco Centenaro, Giorgio Maria Di Nunzio, and    Stefano Tomasin \\ {\small Department of Information Engineering, University of Padova, via G. Gradenigo 6/B, Padova, Italy. first.lastname@dei.unipd.it} }
\date{today}

\begin{document}

\maketitle

\begin{abstract}
We consider the \ac{irlv} problem of deciding if a message coming from a \ac{ue} over a wireless network has been originated from a specific physical area (e.g., a safe room) or not. The detection process exploits the features of the channel over which the transmission occurs with respect to a set of network \acp{bs}. A  \ac{ml} approach is used, based on a \ac{nn}. The \ac{nn} is trained with channel features (in particular, noisy attenuation values) collected by the \acp{bs} for various positions of the \ac{ue} both inside and outside the specific area. By seeing the \ac{irlv} problem as an hypothesis testing problem, we address the optimal positioning of the \acp{bs} for minimizing either the \ac{auc} of the \ac{roc} or  the \ac{ce} between the \ac{nn} output and the labels assigned to the training set. We propose a two-stage \ac{pso} algorithm having as target the  minimization of  the \ac{ce} and  the \ac{roc} \ac{auc}. Through simulations we show that for long training and \ac{nn} with enough neurons the proposed solution achieves the performance of the \ac{np} lemma.
\end{abstract}

\begin{IEEEkeywords}
Physical layer security, location verification, neural network, support vector machine, network planning.
\end{IEEEkeywords}
\glsresetall

\section{Introduction}

Applications using information on the user location are rapidly spreading, also to ensure that some services are obtained only in pre-determined areas, e.g., media streaming, online voting, gambling and location-based social networking. In these and other applications it is important to verify the position of the user rather than simply rely on its claims, that can be easily forged by hacking the software.

Location verification systems aim at verifying the position of devices in a mobile communication network, with applications in sensor networks \cite{Zeng-survey, 8376254}, the Internet of Things \cite{7903611}, and geo-specific encryption solutions \cite{quaglia}. Various approaches have been proposed for location verification, typically based on the measurement of the distance between the transmitter and other users  or  anchor nodes belonging to the network infrastructure. The distance measures are obtained, for example, through the \ac{rss} at anchor nodes for signals transmitted by the terminal to be verified.  This problem is closely related to the {\em user authentication} at the physical layer, where the channel features are exploited to verify the sender of a message over a wireless channel \cite{7270404}.

We focus here on the \ac{irlv} problem, i.e., the problem of deciding whether a message coming from a terminal over a wireless network has been originated from a specific physical area, e.g., a safe room, or not \cite{Zeng-survey}. \ac{irlv} can be seen as an hypothesis testing problem between two alternatives, namely being inside or outside the specific area. Among proposed solutions, we recall distance bounding techniques with rapid exchanges of packets between the verifier and the prover \cite{Brands}, also using radio-frequency and ultrasound signals \cite{Sastry}, and solutions based on the use of anchor nodes and increasing transmit power by the sender \cite{Vora}. More recently, a delay-based verification technique has been proposed  in \cite{7145434}, leveraging geometric properties of triangles, which prevents an adversary from manipulating measured delays.  

In this paper, we consider the \ac{irlv} problem for a \ac{ue} connected to a set of network \acp{bs}. We first observe that the  \ac{np} lemma \cite{Neyman289} provides the optimal solution to the \ac{irlv} hypothesis testing problem by providing  the test with the minimum \ac{md} probability for a given \ac{fa} probability. For more complex situations (including more elaborate channel impairments) the \ac{np} test could not be implemented, and we propose instead a \ac{ml} approach  where i) first channel measurements are collected by trusted nodes both inside and outside the region of interest, ii) then a model is trained to take decisions between the two hypothesis before deployment as tester. \ac{ml} techniques have already found application in the user authentication problem (see  \cite{xiao-2018} and references therein). However, \ac{ml} solutions have never been applied before to the \ac{irlv} problem to the best of authors' knowledge. 

Then, leveraging the  models and results obtained for the \ac{irlv} we address the problem of optimum positioning of the \acp{bs} (network planning) that provides the channel measurements over which the test is performed. Two metrics are considered: the \ac{ce} obtained in \ac{nn} training and the \ac{auc} of the \ac{roc}. We propose a two-stage \ac{pso} algorithm having as target the  minimization of  the \ac{ce} and  the \ac{roc} \ac{auc}. We also investigate the relation between the two metrics, \ac{auc} and \ac{ce}, showing how both are functions of the \ac{fa} and \ac{md} probabilities. Simulation results of the proposed solution over channels with shadow fading complete this work.

%The rest of the paper is organized as follows. In Section \ref{sec:sys model} we describe the system and channel model. Section \ref{sec: ml} describes the optimal \ac{np} solution and the \ac{ml} solution to \ac{irlv}. In Section \ref{sec:bsPos} we discuss the \acp{bs} positioning problem and we show how \ac{ml} can be used to solve it. In Section \ref{sec: nr} we present numerical results showing the effectiveness of the proposed solutions for both \ac{irlv} and \ac{bs} positioning, before conclusions outlined in Section \ref{sec:conc}.

%Network planning, i.e., designing optimal \acp{bs}' layout maximizing a chosen metric, has been studied in the literature in the context of \ac{wsn} \cite{bogdanov2004power} \cite{akkaya2007positioning} and cellular systems \cite{mathar2001optimal} \cite{glasser2005complexity}  \cite{islam2012capacity} \cite{yangyang2004particle}. In \cite{bogdanov2004power} the sensors network is optimized using as metric the rate of transmission between nodes. The resulting optimization is then solved using two heuristic iterative algorithms, since the problem is NP-complete. For a comparison between other proposed approaches and metric choices for \ac{wsn}, see \cite{akkaya2007positioning}. For what concerns cellular systems, in \cite{mathar2001optimal} and \cite{islam2012capacity} the chosen metrics are, respectively, the number of connected users and the cumulative channel capacities. For both these works the solving tools are taken from the integer and mixed-integer programming literature. In \cite{yangyang2004particle}, instead, \ac{pso} is used to optimize \acp{bs}' position w.r.t coverage and deployment costs (proportional to  the number of \acp{bs}).
%To the best of authors' knowledge there are no literature examples of \acp{bs} positions planning that seek to optimize physical layer authentication performance.
 
\section{System Model}\label{sec:sys model}


We consider a cellular system with $N_{\rm AP}$ \acp{bs} covering a region $\mathcal{A}$ over a plane. We propose a \ac{irlv} system able to determine if a \ac{ue} is transmitting from inside an {\em authorized} sub-region $\mathcal{A}_0 \subset \mathcal{A}$. The location dependency of the channel between the \ac{ue} and the \acp{bs} is exploited to distinguish between transmissions from inside $\mathcal{A}_0$ and outside of it, i.e., from  $\mathcal{A}_1=\mathcal{A} \setminus \mathcal{A}_0$.  Transmission is assumed to be narrowband and the channel feature used for \ac{irlv} is the attenuation introduced by the channel.

The \ac{irlv} procedure comprises two phases. In the first phase, named identification, the \ac{ue} transmits a training signal (known at the \acs{bs}) from various points inside region $\mathcal{A}_0$.  The \acp{bs} estimate the channel attenuations and store them in association with $\mathcal{A}_0$. Some external authentication technique on the transmitted packet  ensures that the received signal upon which the attenuation is estimated is actually transmitted from region $\mathcal{A}_0$ at the nominal power. Similarly, attenuation values are collected when the \ac{ue} transmits  from the complementary area $\mathcal{A}_1$ and  stored by the \ac{bs} in association to $\mathcal{A}_1$. In the second  phase, named verification, the \ac{ue} transmits a known training sequence from any point in $\mathcal{A}$ and the \ac{irlv} system must decide whether the \ac{ue} is in region $\mathcal{A}_0$ or $\mathcal{A}_1$.

% The location dependency of the features of the transmission channel can be further enhanced by properly placing the \acp{bs} over the area $\mathcal{A}$. In particular, different positioning lead to different shadowing incurred by the transmission of the \acp{ue} toward the \acp{bs}. Our aim is to find the optimal \acp{bs} positioning such that the authentication system can optimally discriminate between different areas based on the estimated attenuation values.

\subsection{Channel Model}

We denote as $\bm{x}_{\rm ap}^{(n)} =(X_{\rm ap}^{(n)},Y_{\rm ap}^{(n)})$ the position of the $n$-th \ac{bs}. For a \ac{ue} located at $\bm{x}_{\rm ue}=(X_u,Y_u)$, its distance from \ac{bs} $n$ is denoted as $L(\bm{x}_{\rm ue},\bm{x}_{\rm ap}^{(n)})$. When a \ac{ue} transmits with power $P_{\rm tx}$, the received power at the $n$-th \ac{bs} is
\begin{equation}\label{eq: rec pow}
    \left(P_{\rm rc}^{(n)}\right)_{\rm dB}= \left(P_{\rm tx}\right)_{\rm dB} - \left(a^{(n)}\right)_{\rm dB},
\end{equation}
where $a^{(n)}$ is the attenuation incurred by the transmitted signal to \ac{bs} $n$. The attenuation coefficient $a^{(n)}$ includes the effects of path-loss and shadowing. Denoting the path-loss coefficient as $P_{\ell}^{(n)}$  the shadowing component is log-normally distributed $\left(s^{(n)}\right)_{\rm dB} \sim \mathcal{N}(0,\sigma_s^2)$ and
\begin{equation}
    \left(a^{(n)}\right)_{\rm dB} = \left(P_{\ell}^{(n)})\right)_{\rm dB} + \left(s^{(n)}\right)_{\rm dB}.
\end{equation}

The channel model for path loss and shadowing is derived from \cite{3gpp}. For a \ac{los} link the path loss coefficient in dB is modelled as
\begin{equation}\label{eq:los}
    P_{\ell,LOS}^{(n)} = 20\log_{10}\left(\frac{f 4\pi L(\bm{x}_{\rm ue},\bm{x}_{\rm ap}^{(n)})}{c}\right),
\end{equation}
where $f$ is the carrier frequency and $c$ is the speed of light. For a  non-\ac{los} link the path loss coefficient in dB is defined as
\begin{equation}
\begin{split}
    P_{\ell, nLOS}^{(n)} = 40\left(1-4 \cdot 10^{-3}  h_{ap}\right) \log_{10}\left (\frac{L(\bm{x}_{\rm ue},\bm{x}_{\rm ap}^{(n)})}{10^3}\right ) \\
    -18\log_{10}h_{\rm ap}
    + 21\log_{10}\left(\frac{f}{10^6}\right) + 80,
    \end{split}
\end{equation}
where $h_{\rm ap}$ is the \ac{bs} antenna elevation.

We assume that shadowing correlation between points $\bm{x}_i$ and $\bm{x}_j$ is
\begin{equation}\label{eq: coor mat}
    \bm{\Sigma}_{(i,j)} = \sigma_s^2e^{-\frac{L(\bm{x}_i,\bm{x}_j)}{d_c}},
\end{equation}
where $d_c$ is the shadowing decorrelation distance. 


\section{In-region Location Verification}\label{sec: ml}

 \begin{equation}
\begin{split}
  \mathcal H_0: \mbox{the \ac{ue} is in $\mathcal{A}_0$} \\
  \mathcal H_1: \mbox{the \ac{ue} is in $\mathcal{A}_1$}. 
  \end{split}
\end{equation}

During the identification phase a trusted \ac{ue} transmits a training signal with known transmit power from $S$ positions, some inside and other outside the area $\mathcal A_0$. The corresponding $S$ attenuation vectors are noted as $\bm{a}^{(i)}, \ i=1,\dots S$, while for position $i$ the corresponding label is $t_i=1$ if \ac{ue} is in $\mathcal A_1$ and we define $t_i=0$ if \ac{ue} is in $\mathcal A_0$. We collect all the $t_i$ values into the vector $\bm{t}=[t_1,...,t_S]$.

By using these training vectors and labels we aim at building a function
\begin{equation}
    \hat{t} = f(\bm{y}) \in \{0,1\}\,,
\end{equation}
that for an attenuation vector $\bm{y}$ determines if the \ac{ue} is in $\mathcal A_0$ ($\hat{t}=0$) or in $\mathcal A_1$ ($\hat{t}=1$).

The performance of the \ac{irlv} system is assessed in terms of two error probabilities: the \ac{fa} probability, i.e., the probability  that a \ac{ue} in $\mathcal A_0$ is declared outside this area, and the \ac{md} probability, i.e., the probability that a \ac{ue} outside $\mathcal A_0$ is declared inside the area. In formulas, $P_{\rm FA} ={\mathbb P}(\hat{t} =1 | \mathcal H_0)$ and  $P_{\rm MD}={\mathbb P}(\hat{t} = 0 | \mathcal H_1)$.

\subsection{Detector for Known Attenuation Statistics}

The \ac{irlv} problem can be seen as an hypothesis testing problem between the two hypothesis $\mathcal H_0$ and $\mathcal H_1$. When the statistics of the attenuation vectors are known under the two hypotheses, the most powerful test for the \ac{irlv} problem is provided by the \ac{np} lemma. In particular, let us  define the \ac{llr} of the attenuation vector
\begin{equation}\label{eq:lr}
    \mathcal{L}^{(\bm{y})}=\log\left(\frac{p(\bm{y}|\mathcal{H}_0)}{p(\bm{y}|\mathcal{H}_1)}\right)\,,
\end{equation}
where $p(\bm{y}|\mathcal{H}_i)$ if the \ac{pdf} of observing $\bm{y}$ given that hypothesis $\mathcal H_i$ is verified. Then the \ac{np} test function is 
\begin{equation}
\label{eq:thrOpt}
    \hat{t} = f(\bm{y}) = \begin{cases}
    0 & \mathcal{L}^{(\bm{y})} \geq \theta\,, \\ 
    1 & \mathcal{L}^{(\bm{y})} < \theta\,, 
    \end{cases}
\end{equation}
where $\theta$ is a threshold to be chosen in order to ensure the desired \ac{fa} probability. This test ensures that for the given \ac{fa} probability the \ac{md} probability is minimized. 

\subsection{Example of \ac{np} Test}
\label{sec:los}
We now describe an example of application of the \ac{np} test, where we can easily obtain a close-form expression for $f(\bm{y})$. 

Let us define the overall network area as a circle $\mathcal{C}$ with radius $R_{\rm out}$ and consider a single \ac{bs} located at the center of $\mathcal{C}$. Consider $\mathcal{A}_{0}$ as a rectangle with nearest point to the center of $\mathcal{C}$ at a distance $R_{\rm min}$. The outside area is $\mathcal{A}_1 = \mathcal{C} \setminus \mathcal{A}_0$.

In the \ac{los} scenario the attenuation incurred by a \ac{ue} is given by (\ref{eq:los}), which only depends on its relative distance to the \ac{bs}. Considering an attenuation value $a$, the distance $R$ from the \ac{ue} to the \ac{bs} is hence given by 
\begin{equation}
    R = \frac{c a}{f 4 \pi}.
\end{equation}
Therefore instead of considering $p(y|\mathcal H_i)$ we consider $p(R_0|\mathcal H_i)$, where $R_0$ is the distance of the \ac{ue} from the center corresponding to attenuation $y$. In order to obtain this \ac{pdf} we first note obtain the \ac{cdf} at $R_0$ of the distance from the \ac{bs}, i.e.,  the probability that the \ac{ue} is located at a distance $R\le R_0$ in $\mathcal{A}_0$ is
\begin{equation}\label{eq:cdf}
     \mathbb{P}(R \le R_0|\mathcal{A}_0) = \frac{1}{|\mathcal{A}_0|}\int_{R_{\rm min}}^{R_0} \rho \alpha(\rho) d\rho,
\end{equation}
where $\alpha(R)$ denotes the angle of the circular sector measured from a distance $R$ and intersecting area $\mathcal{A}_0$. Then by taking the derivative of the \ac{cdf} (\ref{eq:cdf}) with respect to $R_0$ we obtain the \ac{pdf} 
\begin{equation}\label{eq:num}
    p_{R|\mathcal{A}_0}(R_0|\mathcal{A}_0) = \frac{1}{|\mathcal{A}_0|}R_0\alpha(R_0).
\end{equation}
Following the same reasoning and considering that the length of the circular sector with radius $R_0$ located in $\mathcal{A}_1$ is $2\pi - \alpha(R_0)$, we obtain the \ac{pdf} of transmission from a distance $R_0$ in $\mathcal{A}_1$ as
\begin{equation}\label{eq:den}
     p_{R|\mathcal{A}_1}(R_0|\mathcal{A}_1) = \frac{1}{|\mathcal{A}_1|}R_0\left(2\pi-\alpha(R_0)\right).
\end{equation}
From (\ref{eq:num}) and (\ref{eq:den}) we obtain the \ac{llr} as a function of the \ac{ue}'s distance from the \ac{bs} as 
\begin{equation}
    \mathcal{L}(a)=\log\left(\frac{|\mathcal{A}_1|\alpha(\frac{c a}{f 4 \pi})}{|\mathcal{A}_0|\left(2\pi-\alpha(\frac{c a}{f 4 \pi})\right)}\right).
\end{equation}
%This result will be used in the numerical section part to compare the classifier performance of the \ac{nn} with the \ac{np} test.

\subsection{Neural Network Implementation}\label{sec:nn}

For more complicated scenarios of attenuation statistics, it becomes hard to obtain close-form expressions of the \ac{llr}. Therefore we propose here to use a \ac{ml} approach, where in the identification phase a \ac{nn} is trained in order to classify the attenuation vectors $\bm{a}^{(i)}$ according to the labels $t_i \in \{0,1\}$ and then the \ac{nn} is used on the test attenuation vectors $\bm{y}$ to provide the decision $\hat{t} \in \{0,1\}$. Therefore the \ac{nn} implements the function $\hat{t} = f(\bm{y})$.

A feed-forward \ac{nn} processes the input in stages, named layers, where the output of one layer is the input of the next layer. The input of the \ac{nn} is $\bm{y}^{(0)} = \bm{y}$, and layer $\ell-1$ has $N^{(\ell-1)}$ outputs obtained by processing the inputs with $N^{(\ell-1)}$ functions named neurons. The output of the $n$-th neuron of the $\ell$-th layer is
\begin{equation}\label{eq:nonLin}
y_n^{(\ell)} = \psi\left( \bm{w}_n^{(\ell -1)}\bm{y}^{(\ell-1)}+b_n^{(\ell)} \right),
\end{equation}
where $\bm{w}_n^{(\ell -1)}$ and $b_n^{(\ell)}$ are coefficients to be determined in the training phase, and $\psi(\cdot)$ is a fixed pre-determined activation function. The neuron maps via  $\psi$ a  linear combination with weights $\bm{w}_n^{(\ell -1)}\in \mathbb{R}^{1\times N^{(\ell-1)}}$ of the outputs $\bm{y}^{(\ell-1)} \in \mathbb{R}^{N^{(\ell-1)} \times 1 }$ of the previous layer, plus a bias $b_n^{(\ell)} \in \mathbb{R}^{N^{(\ell-1)} \times 1 }$. The output of the \ac{nn} is the scalar 
\begin{equation}
	\tilde{y} \triangleq \sigma(y^{(L-1)}),	
\end{equation}
where $\sigma(\cdot)$ is the sigmoid function. Finally, the test function is obtained by thresholding $\tilde{y}$, i.e.,
\begin{equation}
\label{eq:decNN}
    f(\bm{y}) = \begin{cases}
    1 & \tilde{y} > \lambda \\
    0 & \tilde{y} \leq \lambda.
    \end{cases}
\end{equation}
Different values of $\lambda$ provide different values of \ac{fa} and \ac{md} probabilities for this \ac{irlv} test.

Various options are considered in the literature for the training of the \ac{nn}. Here we aim at minimizing the \ac{ce} between the output of the \ac{nn} and the labels $t_i$. An estimate of the \ac{ce} over the $S$ training points is 
\begin{equation}\label{eq:ce}
\hatcross = -\frac{1}{S} \sum_{i=1}^{S}\left[t_i\log \tilde{y}_i +\left(1-t_i\right)\log\left(1-\tilde{y}_i\right] \right),
\end{equation}
where $\tilde{y}_i$, $i=1, \ldots, S$, is the soft output corresponding the $i^{\rm th}$  training point and $p_t$ is the \ac{pmd} of label $t$. \hl{This is due to the fact that we can interpret $\tilde{y}$ as $p(\mathcal{H}_0|\bm y)$ {\cite{Bishop2006}}}.
 Vectors $\bm{w}_n^{(\ell)}$ and scalars $b_n^{(\ell)}$ are then adjusted via the gradient descent algorithm. 

Note that this approach does not require the knowledge of the statistics of $\bm{y}$ under the two hypotheses, while instead it requires a large enough set of training points $S$ to converge. In the following we show by simulation that for $S$ large enough we obtain the same performance (\ac{fa} and \ac{md}) probabilities of the \ac{np} solution, in the \ac{los} scenario of the previous section.

% \subsection{Support Vector Machine}\label{sec:svm}
% A \ac{svm} \cite{Bishop2006} is a supervised learning model that can be used for classification and regression. We focus here on binary classification, i.e., we define the identification function as
% \begin{equation}
%   t_i =
%   \begin{cases}
%   -1 \quad \text{if} \quad \bm{y} \in \mathcal{A}_0\\
%   1 \quad \text{if} \quad \bm{y} \in \mathcal{A}_1.
%   \end{cases}
% \end{equation}
% Given the input vector $\bm{y}^{(0)} \in \mathbb{R}^N$ the \ac{svm} returns $\hat{t} = 1$ if $\bm{y}^{(0)}$ belongs to class 0 whereas $\hat{t}=-1$ if $\bm{y}^{(0)}$ belongs to class 1. It comprises the function $\tilde{t}: \mathbb{R}^N \to \mathbb{R}$ defined by
% \begin{equation}
% \label{eq:svm}
% \tilde{t} = \mathbf{w}^T \phi (\mathbf{a}^{(i)}) + b,
% \end{equation}
% where $\phi: \mathbb{R}^N \to \mathbb{R}^K$ is a feature-space transformation function, $\mathbf{w} \in \mathbb{R}^K$ is the weight vector and $b$ is a bias parameter, and the decision function is
% \begin{equation}
% \label{eq:cases}
% \hat{t} = 
% \begin{cases}
% +1 \quad \tilde{t}  \geq \gamma^* \\
% -1 \quad \tilde{t}  < \gamma^*,
% \end{cases}		
% \end{equation} 
% where $\gamma^*$ is a fixed threshold and controls \ac{fa} and \ac{md} probabilities. Note that in the classical \ac{svm} formulation we have $\gamma^* = 0$.

% While the feature-space transformation function is typically fixed, the vector $\mathbf{w}$ must be properly chosen to perform the desired classification


\section{Network Planning}\label{sec:bsPos}

As the attenuation depends on the position of the \acp{bs} and on the surrounding environment, the performance of the authentication system depend on the number of \acp{bs} and on their location in space. In this section, we derive an approach to optimally locate \acp{bs} (network planning) so that the authentication system attains the best performance. 

We have shown so far that the \ac{nn} is an effective tool to perform hypothesis testing. Furthermore it has been shown in \cite{nostro} that the \ac{nn} with a sufficient number of neurons and for a sufficiently large training set achieves the optimal \ac{np} performance. The idea is to exploit the training process of the \ac{nn} in order to obtain a proxy of the performance of the \ac{irlv} system and hence find the optimal location of the \acp{bs}, such that the resulting system attains the best classification perfrormance.

The optimal solution for \acp{bs} positioning in the \ac{irlv} context is given by the minimization of $P_{\rm MD}$ for each $P_{\rm FA}$ value. However, we do not want to fix {\em a priori} the value of $P_{\rm FA}$ while capturing the whole behavior of the \ac{roc}, defined as the  function associating the $P_{\rm MD}$ with the corresponding $P_{\rm FA}$, for all possible values of thresholds $\lambda$. In particular, we consider two metrics, i.e., the   \ac{roc} \ac{auc} \cite{hanley-82} and the \ac{ce}. 

We now describe the two metrics and highlight their relation, by showing how they  are related to the \ac{fa} and \ac{md} probability.

\paragraph{Area Under the ROC Curve} The \ac{roc} \ac{auc} is defined as 
\begin{equation}
    K  = \int_{0}^{1} P_{\rm MD}\left(P_{\rm FA}\right) d P_{\rm FA},
\end{equation}
where $P_{\rm MD}\left(P_{\rm FA}\right)$ is the $P_{\rm MD}$ value as a function of the $P_{\rm FA}$, which corresponds to the integral of the \ac{roc} function. By minimizing the \ac{auc}, we minimize the average \ac{md} probability for a uniform \ac{fa} probability
%\footnote{Notice that traditionally the \ac{auc} is a metric that needs to be maximized \cite{hanley-82}. This is due to the fact that the curve of the system performance computed as in \cite{hanley-82} and \cite{Kennedy-11} is given by the true positive rate vs. the false negative rate value, which is optimal when the true positive rate value is maximized for each false negative rate value. Since we consider as system performance metrics for the authentication system the $P_{\rm MD}$ and the $P_{\rm FA}$ we instead need to minimize the \ac{auc}.}
. However, note that in order to compute the \ac{auc} we must run the \ac{nn} over a set of test attenuation vectors multiple times with different threshold in order to reconstruct the \ac{roc} and compute a numerical approximation of its integral.

\paragraph{Cross-Entropy} Another metric that captures the quality of a test is the \ac{ce} between $t$ and $\tilde{y}$. This metric is immediately provided at the end of the identification phase, where the learning of the \ac{nn} (which is based on the \ac{ce}) is completed. We now provide the relation between the \ac{ce} and the \ac{fa} and \ac{md} probabilities.

As previously proved, {\ac{ce}} training provides, asymptotically, 
\begin{equation}
	\tilde{y} \to p(\mathcal{H}_0|\bm y).	
\end{equation}
From the Bayes rule, 
\begin{equation}
	p(\mathcal{H}_0| \bm y) = \frac{1}{1+\mathcal{L}^{(\bm y)} \frac{p(\mathcal{H}_1)}{p(\mathcal{H}_0)}}	
\end{equation}
Hence, \eqref{eq:decNN} is equivalent to \eqref{eq:thrOpt}, with
\begin{equation}
	\theta = \frac{1-\lambda}{\lambda} \frac{p(\mathcal{H}_0)}{p(\mathcal{H}_1)}	
\end{equation}
 
% Since $t$ and $\hat{t}$ are Bernoulli variables, the \ac{kl} between $t$ and $\hat{t}$ can be written as 
%\begin{equation}
%\mathbb D(p_t; p_{\hat{t}}) = P_{\rm MD} \log \frac{P_{\rm MD}}{1 - P_{\rm FA}} + (1-P_{\rm MD})\log \frac{1- P_{\rm MD}}{P_{\rm FA}}\,,
%\label{KLb}
%\end{equation}
%where $p_t$ is the \ac{pmd} of $t$, $p_{\hat{t}}$ is the \ac{pmd} of $\hat{t}$. Now, since $\hat{t}$ is obtained by $\tilde{y}$, by the data processing inequality we have (see also \cite{Tomasin-Ferrante}) that {\em for any thresholding of $\tilde{y}$}
%\begin{equation}
%\mathbb D(p_t; p_{\hat{t}}) \leq \mathbb D(p_t; p_{\tilde{y}})\,.
%\label{dpi}
%\end{equation}
%Therefore, the right term of the inequality is  a synthetic metric of the test, irrespective of the optimization of the threshold, i.e., of a specific target \ac{fa} probability. The \ac{auc} is also another synthetic description irrespective of the specific thresholding. Now let us establish the connection between \ac{fa} and \ac{md} probabilities and \ac{ce}. The \ac{ce} used for \ac{nn} optimization (\ref{eq:ce})  can also written as 
%\begin{equation}
%\mathbb H(p_t,p_{\tilde{y}}) = \mathbb H(p_t) +\mathbb D(p_t; p_{\tilde{y}})\,,
%\end{equation}
%where $\mathbb H(\cdot)$ is the entropy function and $\mathbb D(\cdot;\cdot)$ is the \ac{kl} divergence. Therefore by  adding $\mathbb H(p_t)$ to both terms in (\ref{dpi}) we have 
%\begin{equation}
%\mathbb H(p_t) +\mathbb  D(p_t; p_{\hat{t}}) \leq \mathbb H(p_t,p_{\tilde{y}})\,. 
%\end{equation}
%Since the entropy of $t$ is the same for all testing techniques, minimizing the \ac{ce} is equivalent to minimizing an upper bound on the \ac{kl} divergence between $t$ and $\hat{t}$, which can be written as a function of \ac{fa} and \ac{md} probabilities by (\ref{KLb}). This justifies the use of the \ac{ce} as a metric for the optimization of the \ac{bs} position.

\subsection{Particle Swarm Optimization}

In order to solve the \acp{bs} positioning problem we exploit the \ac{pso} \cite{Kennedy-11}, briefly recalled here.
 
\ac{pso} is an iterative optimization algorithm based on social behavior of animals, e.g., birds flocking and fish schools. Consider a total number of $P$ particles, where  particle $p=1, \ldots P$, is described by a vector of \acp{bs} positions $\bm{x}_p = [\bm{x}_{\rm ap}^{(1)}(p),\ldots, \bm{x}_{\rm ap}^{(N_{\rm ap})}(p)]$ and by its velocity $\bm{v}_p$.  Each particle is a candidate solution of the optimization problem. 

Starting from a random position for all the particles, at each iteration both  positions $\bm{x}_p$ and  velocities $\bm{v}_p$ are updated. Two optimal values are defined in each iteration: the global optimum found so far by the entire population and a local optimum for each particle, i.e., the optimal value found by the individual $p$ up to the current iteration. We define as $\bm{o}_g$ the position of the the global optimal values and as $\bm{o}_p$ the position of the optimal value found by particle $p$ at the current iteration. According to the chosen objective function, the optimal values are those minimizing either the \ac{roc} \ac{auc} or  the \ac{ce}.

The position and velocity of the particles are updated at iteration $\ell$ as \cite{Kennedy-11}
   \begin{equation}\label{eq: v up}
\begin{split}
  \bm{v}_p(\ell) = \omega \bm{v}_p(\ell-1)+\phi_1(\ell)(\bm{o}_p(\ell-1)-\\
  -\bm{x}_p(\ell-1))+\phi_2(\ell)(\bm{o}_g(\ell-1)-\bm{x}_p(\ell-1));
  \end{split}
  \end{equation}
  \begin{equation}\label{eq: p up}
  \bm{x}_p(\ell) = \bm{x}_p(\ell-1) + \bm{v}_p(\ell);
 \end{equation}
where $\omega$ is the inertia coefficient and $\phi_1$ and $\phi_2$ are random variables uniformly distributed in $[0,c_1]$ and $[0,c_2]$, respectively, where $c_1$ and $c_2$ are defined as acceleration constants. The values of the inertia coefficient and of the acceleration constants are the parameters of the \ac{pso} problem.

\subsection{PSO-Based Network Planning}

As we have seen the \ac{roc} \ac{auc} well describes the overall behaviour of the \ac{roc} and is hence widely recognized as a valid synthetic metric for hypothesis testing. The \ac{ce} represents instead only an upper bound on the \ac{kl} divergence, thus is less reliable and is not directly related to the \ac{auc}. On the other hand, computation of the \ac{auc} is complicated by the need to performing extensive testing, while the \ac{ce} is immediately provided by the \ac{nn} training process. 

In particular, the testing needed to compute \ac{auc} has an additional complexity (with respect to training that must be performed anyway), of 
\begin{equation}
    \mathcal{C}_{\rm test} = P \left( \mathcal{C}_{\rm out}+\mathcal{C}_{\rm ROC}+\mathcal{C}_{\rm AUC}\right),
\end{equation}
where  $\mathcal{C}_{\rm out}$ denotes the complexity associated to running the \ac{nn} on the test points, $\mathcal{C}_{\rm ROC}$ denotes the complexity of building the \ac{roc} function and $\mathcal{C}_{\rm AUC}$ denotes the complexity of integrating the \ac{roc}. The \ac{nn} running cost $\mathcal{C}_{\rm out}$ is given by the total number of multiplication and additions needed to compute the output value $y^{(L-1)}$ for all testing vectors, i.e.,
\begin{equation}
    \mathcal{C}_{\rm out} = \left(2N_{\rm AP}N_{\rm h}+2N_{\rm h}^2N_{\rm L} + 2N_{\rm h}\right)\tau ,
\end{equation}
where $N_{\rm h}$ is the number of neurons in the hidden layer, $N_{\rm L}$ is the number of hidden layers and $\tau$ is the size of the testing set.
The computation of the \ac{roc} curve requires the estimation of the $P_{\rm FA}$ and $P_{\rm MD}$ values for each threshold value $\lambda$, whereas the computation of the \ac{auc} requires the numerical integration of the \ac{roc} curve over $P_{\rm FA}$ values.

In order to reduce the complexity of the \ac{pso} algorithm and still approximate the optimal solution in terms of the \ac{auc} metric we propose to split the \ac{pso} algorithm into two stages: for a first round of iterations the position of the \acp{bs} is moved by minimizing the \ac{ce}, and starting from the results of these iterations a second round  of iterations is started, where we use the \ac{auc} as optimization function. 
The proposed two-stage algorithm for \acp{bs} positioning is reported in Algorithm 1. We initialize $P$ particles with random positions for each of the $N_{\rm AP}$ \acp{bs} in each particle; we train the \ac{nn} and compute the \ac{ce} $\rm{CE}_p^{(0)}$ for each particle $p$. The global optimum value $\rm{CE}_g$ is set to the minimum among all $\rm{CE}_p^{(0)}$ values. Then, the positions and the velocities of the particles are updated via (\ref{eq: v up}) and (\ref{eq: p up}), and both the local and global optimum are updated according to the obtained values at the current iterations. When $\rm{CE}_g$ converges the objective function is set to the \ac{auc} and all particles perform  training and testing of the \ac{nn}. The algorithm stops when the global optimum converges.

 \begin{algorithm}[b!]

\small

  \KwData{ number of particles $P$, $N_{\rm AP}$}
  \KwResult{optimal position }
  Initialize particles\;
  train the \ac{nn} algorithm for each particle $\to$ $\rm{CE}_p^{(0)}$, $p=1,...,N_p$\;
  $\rm{CE}_g=\underset{p=1,...,N_p}{min} \, \rm{CE}_p^{(0)}$\;
  $it = 0$\;

  \Repeat{convergence of $\rm{CE}_g$}{
         $i = i + 1$\;
         \For{$p=1,\ldots,P$}{
         update velocity and position vector of particle via (\ref{eq: v up}) and (\ref{eq: p up})\;
                  train the \ac{nn} for each particle $\to$ $\rm{CE}_p^{(i)}$\;
                  \If{$\rm{CE}_p^{(i)} < \rm{CE}_g$}{ $\rm{CE}_g = \rm{CE}_p^{(i)}$ \;}
         }
      
      }
      
      objective function $\to$ \ac{auc}\;
      $K_g = 1$\;
      $i = 0$\;
      \Repeat{convergence of $K_g$}{
         $i = i + 1$\;
         \For{$p=1,\ldots,P$}{
         update velocity and position vector of particle via (\ref{eq: v up}) and (\ref{eq: p up})\;
                  train the \ac{nn} for each particle \;
                  test the \ac{nn} $\to$
                  $K_p^{(i)}$\;
                  \If{$K_p^{(i)} < K_g$}{ $K_g = K_p^{(i)}$ \;}
         }
      
      }
    
\caption{Proposed two-stage APs positioning algorithm.}
 \end{algorithm}

%Notice that, as the optimization problem is non-convex, solving \ac{pso} is similar to a multi-start optimization considering $P$ different starting points, which is a standard method used to avoid local minimum solutions. As the number $P$ increases the probability of resolving to a local solution is reduced.

\section{Numerical Results}\label{sec: nr}

We first compare the \ac{nn} solution with the optimal \ac{np} test in the \ac{los} scenario, using a single \ac{bs} as described in Section \ref{sec:los}. We consider a \ac{nn} with $N_L=3$ hidden layers and different number $N_h$ of neurons in the hidden layer. A sigmoid activation function at the hidden layer and a tanh activation function at the output layer have been used, with $10^6$ training points  and $10^6$ testing points. 

Fig. \ref{fig:NP_comp} shows the \ac{roc} obtained with the \ac{np} test and with the \ac{nn}. We see that, even with a small number of neurons, in this simple problem the \ac{nn} achieves the same \ac{roc} of the \ac{np} test. This is already an indication that the \ac{nn} with sufficiently long training and a large enough number of neurons achieves optimal performance. Further investigation on the connection between these two approaches are left for future  investigation.
 
 \begin{figure}[h]
     \centering
     \includegraphics[width=0.9\columnwidth]{FA_MD_LOS.eps}
     \caption{\ac{roc} of the \ac{np} test and the the proposed \ac{ml} test,  with different number of neurons in the hidden layer of the \ac{nn}.}
     \label{fig:NP_comp}
 \end{figure}

 
In order to test the \ac{nn} approach in a more challenging scenario, we consider a network with $N_{\rm AP}=5$ \acp{bs}, each  characterized by a different  shadowing map, obtained with the model described in Section II, for a square area with two streets dividing the square into four quadrants with buildings, and the safe area $\mathcal A_0$  located in quadrant, inside the building.

%Fig. \ref{fig:trueMap} shows a realization of the path loss and shadowing map for an \ac{bs} located in the center of the area $\mathcal{A}$. 
%We further assume that inside $\mathcal{A}$ there are  two orthogonal \ac{los} paths traversing the center of $\mathcal{A}$, representing for example streets surrounded by buildings. We indicate region $\mathcal A_0$   by the red line.


% \begin{figure}[t]
%     \centering
%     \includegraphics[width=0.9\columnwidth]{surfColorato.png}
%     \caption{Example of a realization of the attenuation map in the non-\ac{los} scenario considering only the shadowing effects.}
%     \label{fig:trueMap}
% \end{figure}

Fig. \ref{fig:n_train} shows the average (over shadowing realizations) \ac{roc} of the proposed \ac{nn} \ac{irlv} system  trained with different number of training points $q$. Results have been obtained for a $N_L=3$ layer \ac{nn} with $N_h=5$ neurons in the hidden layer. We see that the \ac{auc} decreases when increasing the number of training points and that, starting from $q=10^6$, the \ac{roc} does not improve. This is due to the the fact that, for the selected \ac{nn} architecture, training has reached convergence and hence adding further data to the training process does not add information to the function implemented by the \ac{nn}. 

\begin{figure}[t]
    \centering
    \includegraphics[width=0.9\columnwidth]{mean_maps_2.eps}
    \caption{\ac{roc} of the \ac{nn} \ac{irlv} system trained with $q$ training points.}
    \label{fig:n_train}
\end{figure}

\subsection{Network Planning Performance}

We consider now the network planning using the proposed Algorithm 1. We consider a \ac{pso} with $P=6$ particles, each composed by a set of $N_{\rm AP}=5$ \acp{bs} initialized with random positions. Though there exists a variety of implementations of the \ac{pso} the most general case for the parameters initialization is given by \cite{clerc2002}, i.e.,   $\omega=0.7298$, $c_1=c_2=1.4961$. Results are averaged over different shadowing realizations. We consider $10^4$ training  points and  $10^3$ testing points. We compare the performance of Algorithm 1 with two conventional \ac{pso} algorithms, using only one optimization metric between \ac{auc} and \ac{ce}. The overall performance metric is the \ac{auc}.

Fig. \ref{fig:CEvsAUC} shows the average \ac{auc} value vs. the number of \ac{pso} iterations. We see that the proposed solution reaches the lowest value of \ac{auc} with respect to the other solution. Moreover, it also achieves the minimum with a lower number of iteration, thus with a lower complexity, as discussed in the previous section. Indeed, Algorithm 1 outperforms even the \ac{pso} with pure \ac{auc} target function, since in general the \ac{pso} does not achieve the optimal solution, but typically stops at locally optimal solutions. On the other hand, the \ac{pso} using only the \ac{ce} as target function shows the highest values of \ac{auc}. 

 
Fig. \ref{fig:cdf} shows the \ac{cdf} of the number of  iterations using \ac{ce} as objective function in Algorithm 1. We note that in half of the cases only four iterations are performed using \ac{ce} and then the rest of the iterations are using the \ac{auc} as a target function. Since from Fig. \ref{fig:CEvsAUC} we note that in five  iterations Algorithm 1 is on average already quite close to convergence we can conclude that the number of iterations in the \ac{auc} stage is very small in most cases. This does not occur for the pure \ac{auc} solution, where a higher number of \ac{auc} iterations is needed to achieve convergence (see  Fig. \ref{fig:CEvsAUC}).
%\balance
\begin{figure} 
    \centering
    \includegraphics[width=0.9\columnwidth]{CE_vsAUC.eps}
    \caption{Mean \ac{auc} vs. the number of \ac{pso} iterations for Algorithm 1, and two \ac{pso} algorithms using only the  \ac{auc} and the \ac{ce} as objective functions. }
    \label{fig:CEvsAUC}
\end{figure}

\begin{figure} 
    \centering
    \includegraphics[width=0.9\columnwidth]{cdf_bar.eps}
    \caption{CDF of the number of iterations of the first stage of Algorithm 1. }
    \label{fig:cdf}
\end{figure}
 
\section{Conclusions}
\label{sec:conc}
%\balance

In this paper we formulated the \ac{irlv} problem as an hypothesis testing problem and  proposed a \ac{ml}  solution. We showed that the \ac{nn} implementation achieves the same performance of the optimal  \ac{np} test for a simple case, and we assessed the effects of the training set size over the \ac{roc}. We then proposed a \ac{pso}  algorithm for the  optimal \ac{bs} positioning, establishing the connection of two objective functions with the \ac{roc}. We than showed by numerical results the effectiveness of the proposed solution in terms of \ac{auc} of the \ac{roc}.


\renewcommand*{\bibfont}{\footnotesize}

\printbibliography

\end{document}
